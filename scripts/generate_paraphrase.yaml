model:
  model_name: 
inference:
  # beam-search: 
  #   beam-size: 5
  #   max-tokens: 2048
  sampling-params:
    temperature: 0.7
    top_p: 0.95
    max_tokens: 4096
    n: 1
data:
  task: generate_paraphrase
  source_dataset: 

  test-set-path: data/process_data/
  output-path: 
