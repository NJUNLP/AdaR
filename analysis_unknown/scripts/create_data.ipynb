{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_origin = \"/home/nfs02/laizj/experiment/uncertainty_analysis/analysis_unknown/data/gsm8k-train.json\"\n",
    "data_path_generate = \"/home/nfs02/laizj/experiment/uncertainty_analysis/analysis_unknown/data/disturbed_qa.jsonl\"\n",
    "output_path = \"/home/nfs02/laizj/experiment/uncertainty_analysis/analysis_unknown/data/gsm8k-train-disturbed.json\"\n",
    "\n",
    "prompt = \"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request. Output each step in a separate line, and explicitly state the final answer after the final step following the format \\\"The answer is\\\".\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Response: \n",
    "Letâ€™s think step by step.\"\"\"\n",
    "\n",
    "with open(data_path_origin, 'r') as f:\n",
    "    origin_data = json.load(f)\n",
    "    \n",
    "with open(data_path_generate, 'r') as f:\n",
    "    generate_data = f.readlines()\n",
    "    generate_data = [json.loads(x) for x in generate_data]\n",
    "    \n",
    "result = []\n",
    "for generate_data_item in generate_data:\n",
    "    origin_query = origin_data[generate_data_item['id']]['prompt']\n",
    "    if abs(len(generate_data_item['query']) - len(origin_query)) / len(origin_query) < 0.2:\n",
    "        # generate_data_item['prompt'] = prompt.format(generate_data_item['query'])\n",
    "        generate_data_item['prompt'] = generate_data_item['query']\n",
    "        result.append(generate_data_item)\n",
    "        \n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(result, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/nfs05/laizj/model/models--Qwen--Qwen2.5-Math-7B-Instruct/snapshots/ef9926d75ab1d54532f6a30dd5e760355eb9aa4d\")\n",
    "input_path = \"/home/nfs02/laizj/experiment/uncertainty_analysis/analysis_unknown/data/gsm8k-train.json\"\n",
    "output_path = \"/home/nfs02/laizj/experiment/uncertainty_analysis/analysis_unknown/generated/generate_template_and_code_gsm8k-train.json\"\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "Task Description:\n",
    "Create:\n",
    "A variable substitution template\n",
    "A Python script generator that converts query responses to executable code\n",
    "Input Format:\n",
    "Query: [Original query with specific values]\n",
    "COT Response: [Chain-of-thought response explaining the solution]\n",
    "Output Requirements:\n",
    "Template:\n",
    "Replace concrete values in the query with <variable_name> placeholders\n",
    "Maintain original structure while making it generic\n",
    "Python Script:\n",
    "Start with variable definitions matching the template placeholders\n",
    "Convert the COT logic to executable Python code\n",
    "Include a final output statement\n",
    "Preserve variable naming consistency with the template\n",
    "\n",
    "### Query:\n",
    "A rectangle has a length of 3 inches and a width of 6 inches. A square has a width of 5 inches. What is the difference in area between the two shapes?\n",
    "\n",
    "### Response:\n",
    "The area of the rectangle is 3 x 6 = <<3*6=18>>18 sq in\n",
    "The area of the square is 5 x 5 = <<5*5=25>>25 sq in\n",
    "The difference is 25 - 18 = <<25-18=7>>7 sq in\n",
    "#### 7\n",
    "\n",
    "### Template:\n",
    "A rectangle has a length of <length> inches and a width of <width> inches. A square has a width of <square_side> inches. What is the difference in area between the two shapes?\n",
    "\n",
    "### Python Code:\n",
    "```python\n",
    "# Variable definitions\n",
    "length = 3\n",
    "width = 6\n",
    "square_side = 5\n",
    "\n",
    "# Calculations\n",
    "rectangle_area = length * width\n",
    "square_area = square_side * square_side\n",
    "difference = square_area - rectangle_area\n",
    "\n",
    "# Output\n",
    "print(difference)\n",
    "```\n",
    "\n",
    "### Query:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\n",
    "\"\"\"\n",
    "\n",
    "with open(input_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "results = []\n",
    "for idx, item in enumerate(data):\n",
    "    \n",
    "    messages = [\n",
    "    {\"role\": \"system\", \"content\": \"\"},\n",
    "    {\"role\": \"user\", \"content\": prompt.format(item[\"prompt\"], item[\"chosen\"])}\n",
    "    ]\n",
    "    \n",
    "    results.append({\n",
    "        \"id\": idx,\n",
    "        \"query\": item[\"prompt\"],\n",
    "        \"prompt\": tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=True),\n",
    "        \"answer\": item[\"answer\"]\n",
    "    })\n",
    "\n",
    "with open(output_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "laizj-torch2.5-py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
